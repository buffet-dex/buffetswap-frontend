#!/usr/bin/env python3
import argparse
import argparse
import boto3
import datetime
import gzip
import hashlib
import json
import logging
import mimetypes
import os
import subprocess
import sys
import tempfile

from colorlog import ColoredFormatter

EXT_TO_GZIP = [".css", ".js", ".otf", ".svg", ".html", ".json"]
LAST_FILES = ["service-worker.js", "index.html"]


def setup_logging():
    LOG_LEVEL = logging.INFO
    LOG_FORMAT = (
        "%(log_color)s%(levelname)s%(reset)s: %(log_color)s%(message)s%(reset)s"
    )
    fmt = ColoredFormatter(LOG_FORMAT)
    stream = logging.StreamHandler()
    stream.setLevel(LOG_LEVEL)
    stream.setFormatter(fmt)
    log = logging.getLogger()
    log.setLevel(LOG_LEVEL)
    log.addHandler(stream)


def list_s3_objects(client, bucket):
    logging.debug(f"Listing objects of bucket [{bucket}]...")

    results = {}
    continuation_token = None
    while True:
        resp = (
            client.list_objects_v2(Bucket=bucket, ContinuationToken=continuation_token)
            if continuation_token
            else client.list_objects_v2(Bucket=bucket)
        )

        for c in resp.get("Contents", []):
            key = c["Key"]
            etag = c["ETag"]
            results[key] = etag

        continuation_token = resp.get("NextContinuationToken", None)
        if not continuation_token:
            break

    return results


def can_skip_upload(src_path, dst_path, s3_objects):
    etag = s3_objects.get(dst_path, "")
    if not etag:
        return False

    hasher = hashlib.md5()
    with open(src_path, "rb") as f:
        hasher.update(f.read())

    return hasher.hexdigest() == etag.strip('"')


def copy_to_s3(client, bucket, src_path, dst_path, no_cache, is_gzip):
    content_type, _ = mimetypes.guess_type(dst_path)

    args = dict()
    args["Bucket"] = bucket
    args["Key"] = dst_path

    if content_type:
        args["ContentType"] = content_type
    if is_gzip:
        args["ContentEncoding"] = "gzip"
    if no_cache:
        args["CacheControl"] = "no-cache,no-store"
        args["Expires"] = "0"

    logging.info(f"Copy {src_path} to {dst_path} as {content_type}...")
    with open(src_path, "rb") as f:
        args["Body"] = f
        client.put_object(**args)


def deploy_to_s3(dist, bucket, cloudfront_id, force=False):
    if bucket.startswith("s3://"):
        bucket = bucket[5:]

    src_prefix_len = len(dist) + 1

    client = boto3.client("s3")
    s3_objects = {}
    if not force:
        s3_objects = list_s3_objects(client, bucket)
        logging.info(f"Found {len(s3_objects)} objects in bucket {bucket}.")

    src_files = []
    for cur_dir, _, file_names in os.walk(dist):
        for fn in file_names:
            if cur_dir == dist and fn in LAST_FILES:
                continue

            if fn == ".DS_Store" or fn.endswith(".map"):
                continue

            src_files.append(os.path.join(cur_dir, fn))

    for fn in LAST_FILES:
        src_files.append(os.path.join(dist, fn))

    skip_count = 0
    copy_count = 0

    with tempfile.TemporaryDirectory() as td:
        for path in src_files:
            if not os.path.exists(path):
                continue
            aws_path = path[src_prefix_len:]
            no_cache = aws_path in LAST_FILES

            _, ext = os.path.splitext(path)
            if ext in EXT_TO_GZIP:
                with open(path, "rb") as f_in:
                    gz_path = os.path.join(td, os.path.basename(path))
                    data = f_in.read()
                    gz_data = gzip.compress(data, mtime=0)
                    with open(gz_path, "wb") as f_out:
                        f_out.write(gz_data)

                    path = gz_path
                    if can_skip_upload(path, aws_path, s3_objects):
                        skip_count += 1
                        continue

                    copy_to_s3(client, bucket, path, aws_path, no_cache, True)
                    copy_count += 1
            else:
                if can_skip_upload(path, aws_path, s3_objects):
                    skip_count += 1
                    continue

                copy_to_s3(client, bucket, path, aws_path, no_cache, False)
                copy_count += 1

    if skip_count > 0:
        logging.info(f"Skipped {skip_count} identical files.")

    if copy_count > 0:
        logging.info(f"Invalidating CloudFront {cloudfront_id} ...")

        inv_items = [
            "/",
            "/asset-manifest.json",
            "/favicon.png",
            "/index.html",
            "/logo-192.png",
            "/logo-512.png",
            "/manifest.json",
            "/service-worker.js",
            "/static/*",
        ]

        cf_client = boto3.client("cloudfront")
        resp = cf_client.create_invalidation(
            DistributionId=cloudfront_id,
            InvalidationBatch={
                "Paths": {
                    "Quantity": len(inv_items),
                    "Items": inv_items,
                },
                "CallerReference": f"Deployment at {datetime.datetime.utcnow().isoformat('T')}Z",
            },
        )

        status = resp["Invalidation"]["Status"]
        logging.info(f"Created cloudfront invalidation. Status: {status}")

    logging.info(f"Deployed to {bucket}.")


def main(args):
    deploy_to_s3("./build", args.bucket, args.cloudfront_id, args.force)


if __name__ == "__main__":
    setup_logging()

    if os.path.abspath(os.path.dirname(sys.argv[0])) != os.path.abspath(os.getcwd()):
        logging.error(
            "You must execute %s in its directory." % os.path.basename(sys.argv[0])
        )
        sys.exit(1)

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--bucket", dest="bucket", required=True, help="S3 bucket. e.g. s3://my-bucket"
    )
    parser.add_argument(
        "--cloudfront-id", dest="cloudfront_id", required=True, help="CloudFront ID"
    )
    parser.add_argument(
        "--force", dest="force", action="store_true", help="CloudFront ID"
    )

    args = parser.parse_args()
    main(args)
